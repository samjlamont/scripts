{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "355b7e5e",
   "metadata": {},
   "source": [
    "## 1) Import some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871fc493-92ec-4a4c-87f5-c591ca1ffbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "from dask.distributed import Client\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "opts.defaults(opts.Curve(color='blue', height=500, width=650, bgcolor='lightgray', show_grid=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be121e7-f041-4715-9abc-4e5bbf32f9d2",
   "metadata": {},
   "source": [
    "I have found Dask's distributed client to greatly improve performance (even locally).<br> \n",
    "\n",
    "You can start a client and go to the dashboard link to see the status of workers and tasks when code is running.<br> \n",
    "\n",
    "We can also create a dask cluster and connect to it through the client to scale compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b74e8-fa4a-44dc-a15f-92a79b762195",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75626913-0b1e-48c4-a603-978e70adfc91",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2) Converting LISFLOOD output to Zarr format for each region (for informational purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80fa698-1904-463e-af16-9c2a5b8143c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lisflood_dataset_region2():\n",
    "    \n",
    "    basedir = \"/home/jovyan/shared/lisflood/ColumbiaUniv/\"\n",
    "    \n",
    "    file_list = [basedir + \"Region02/outputs_cal1/dis.nc\",\n",
    "                 basedir + \"Region02/outputs_cal2/dis.nc\",\n",
    "                 basedir + \"Region02/outputs_val1/dis.nc\",\n",
    "                 basedir + \"Region02/outputs_val2/dis.nc\"]\n",
    "\n",
    "    # Define a function to trim the overlapping times\n",
    "    def preprocess(ds):\n",
    "        if ds.encoding[\"source\"] == f\"{file_list[2]}\":\n",
    "            return ds\n",
    "        elif ds.encoding[\"source\"] == f\"{file_list[3]}\":\n",
    "            return ds.loc[dict(time=slice(\"2012-01-01 00:00\", \"2013-12-31 23:00\"))]\n",
    "        elif ds.encoding[\"source\"] == f\"{file_list[0]}\":\n",
    "            return ds.loc[dict(time=slice(\"2014-01-01 00:00\", \"2016-06-30 23:00\"))]\n",
    "        elif ds.encoding[\"source\"] == f\"{file_list[1]}\":\n",
    "            return ds.loc[dict(time=slice(\"2016-07-01 00:00\", \"2018-12-31 23:00\"))]\n",
    "\n",
    "    # Open all at once, calling the trim function at the same time\n",
    "    ds_all = xr.open_mfdataset(\n",
    "        file_list,\n",
    "        preprocess=preprocess,\n",
    "        chunks={\"time\": 3000, \"lat\": 100, \"lon\": 100},  # CHUNKING HAS A BIG EFFECT!\n",
    "        parallel=True,\n",
    "    )\n",
    "    \n",
    "    # Now clip to reg2\n",
    "    gdf_reg = gpd.read_file(\"/home/jovyan/shared/flood_dev/sam/lisflood/regions_mask_diss.geojson\")\n",
    "    gdf_reg = gdf_reg[gdf_reg.DN == 2]\n",
    "    ds_all.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "    ds_all = ds_all.rio.clip(gdf_reg.geometry, \"EPSG:4326\", all_touched=False)\n",
    "\n",
    "    return ds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d150712b-9af9-4209-9aa6-3197a5f81bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Add some Zarr compression to reduce file size\n",
    "compressor = zarr.Blosc(cname='zstd', clevel=3)\n",
    "encoding = {vname: {\"compressor\": compressor} for vname in ds_chnk.data_vars}\n",
    "\n",
    "# Write to NFS\n",
    "ds_chnk.to_zarr(\"/home/jovyan/shared/flood_dev/sam/lisflood/zarr_test/lisflood_v2_region1_dis_clip.zarr\", encoding=encoding, consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a3021-8d24-4f7c-984a-398b7a41bd70",
   "metadata": {},
   "source": [
    "## 3) Now push to Zarr file (directory) to GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f95dbd0-8858-437b-afe6-5682a33e0de1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### You can write Zarr files directly to a GCS bucket from xarray but I've found it's much faster to use gsutil\n",
    "ex: `gsutil -m cp lisflood_v2_region1_dis_clip.zarr gs://sam-temp-dev/lisflood_region1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e03dba-4f7a-42fc-ae50-f9d16d565b8d",
   "metadata": {},
   "source": [
    "## 4) Now we can access the data directly from the GCS bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3270e14e-d543-4c11-ab1f-65a482c3af22",
   "metadata": {},
   "source": [
    "#### NOTE: I've noticed it's much faster if you disconnect from the VPN first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243d43d6-2db8-491b-b346-c0f4996951a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading data, you can start a new client with a different configuration for better performance\n",
    "# client.close()\n",
    "client = Client(n_workers=4, threads_per_worker=4, memory_limit='4GB', processes=False)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dc49ba-dd5f-43cf-a3a9-46b42f7b0081",
   "metadata": {},
   "source": [
    "This example shows the difference in performance in reading the data based on two different chunking schemes.  \n",
    "\n",
    "The first zarr store is chunked in the lat/lon dimensions as well as the time dimension (the data itself is a stack of 2-D grids).<br> This gives reasonable read performance in requesting the entire time series at a single point, as well as slicing a single time step across all points.\n",
    "\n",
    "The second is only chunked in the lat/lon dimensions while keeping the time series in one huge chunk.<br>  This makes it faster to retrieve the entire time series at a single point, but causes memory problems when trying to slice a single time step.\n",
    "\n",
    "The chunking scheme should be created to optimize the way data will be retrieved.  Zarr is nice for this because it's so flexible.<br>\n",
    "You can try accesing the data with the two different chunking schemes below to see the difference in performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7b481f-79bc-4d91-9460-f2bf24b59649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunked_by_time_and_latlon = \"gs://sam-temp-dev/lisflood_hokkaido_v2\"\n",
    "chunked_by_latlon_only = \"gs://sam-temp-dev/lisflood_hokkaido_v2_2\"\n",
    "\n",
    "ds_dis = xr.open_zarr(chunked_by_time_and_latlon, consolidated=True) \n",
    "ds_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c596795-09f4-452f-bde6-94a52e91789f",
   "metadata": {},
   "source": [
    "#### Here we request the entire time series at a single point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1e0d2c-7122-429b-9a9e-3bf43fd93cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# NOTE: DISCONNECT FROM VPN FOR LOCAL PROCESSING!!\n",
    "\n",
    "# region 4 (Hokkaido)\n",
    "lat = 43.49516727\n",
    "lon = 141.89472086\n",
    "# lat = [43.49516727, 44.99505638]\n",
    "# lon = [141.89472086, 141.79640070]\n",
    "\n",
    "# # region2 (okayama)\n",
    "# lat = 34.61490570\n",
    "# lon = 133.96546034\n",
    "\n",
    "ds_dis.dis.sel(lat=lat, lon=lon, method=\"nearest\").hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f91eb3-923f-4949-afe0-1a9818e1454f",
   "metadata": {},
   "source": [
    "#### Here we slice one array at a single time step, or you can even request several time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb8ac6-fda4-454c-86fb-e3299e6251ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get a single time step\n",
    "# ds_dis.dis.sel(time=\"2011-09-03 16:00\").hvplot.quadmesh(x='lon',\n",
    "#                                                         y='lat',\n",
    "#                                                         title='dis',\n",
    "#                                                         geo=True,\n",
    "#                                                         width=650,\n",
    "#                                                         height=600,\n",
    "#                                                         rasterize=True,\n",
    "#                                                         project=True,\n",
    "#                                                         cmap=\"bmw\",\n",
    "#                                                         clim=(0, 500),\n",
    "#                                                         tiles='EsriImagery')\n",
    "# Get several time steps\n",
    "ds_dis.dis.sel(time=slice(\"2011-09-02 16:00\", \"2011-09-03 16:00\")).hvplot.quadmesh(x='lon',\n",
    "                                                                                   y='lat',\n",
    "                                                                                   title='dis',\n",
    "                                                                                   geo=True,\n",
    "                                                                                   width=650,\n",
    "                                                                                   height=600,\n",
    "                                                                                   rasterize=True,\n",
    "                                                                                   project=True,\n",
    "                                                                                   cmap=\"bmw\",\n",
    "                                                                                   clim=(0, 500),\n",
    "                                                                                   tiles='EsriImagery')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d50e68a-64fd-4a38-b301-311e43282f9e",
   "metadata": {},
   "source": [
    "## 5) Historical JMA as Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_jma = xr.open_zarr(\"gs://sam-temp-dev/320-420-201201-2\", consolidated=True)\n",
    "ds_jma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9697935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lat = ds_jma.latitude.values\n",
    "lon = ds_jma.longitude.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e38d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_jma.rainrate.isel(time=slice(600, 700)).hvplot.quadmesh(x='longitude',\n",
    "                                            y='latitude',\n",
    "                                            title='rainrate',\n",
    "                                            geo=True,\n",
    "                                            width=650,\n",
    "                                            height=600,\n",
    "                                            rasterize=True,\n",
    "                                            project=True,\n",
    "                                            cmap=\"bmy\",\n",
    "                                            clim=(0, 2),\n",
    "                                            tiles='EsriImagery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_jma.rainrate.sel(time=\"2012-01-01 01:00:00\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1c338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds.rainrate.sum(dim=\"time\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds.rainrate.sum(dim=\"time\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af66d6-8664-4e45-b9dc-abd3a558f389",
   "metadata": {},
   "source": [
    "## 6) Reading parquet files with dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0562b9b-e51a-4add-a8e4-40b9e1fd369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59420c57-acc4-4cfa-a1dc-844dd2d99326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import dask.dataframe as dd\n",
    "ddf_bo = dd.read_parquet(\"gs://sam-temp-dev/parquet/test_grid_1_baseline.parquet\")  # ~32 MB vs. 250 MB .xyz\n",
    "ddf_bo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57191d1-6bb3-47e0-8ebd-b18643b84ec3",
   "metadata": {},
   "source": [
    "## 7) Reading and Visualizing SCHISM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf6a52b-584e-4a2a-ab60-ad3b96755bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datashader.transfer_functions as tf\n",
    "import datashader.utils as du\n",
    "from datashader.colors import inferno, viridis\n",
    "import datashader as dsh\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "from holoviews.operation.datashader import datashade, dynspread, rasterize\n",
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib import cm\n",
    "\n",
    "gv.extension('bokeh')\n",
    "gv.output(size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de0c08-70a8-4bb8-87b3-9b2c61079462",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "client = Client(n_workers=4, threads_per_worker=4, memory_limit='4GB', processes=False)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b627ecc5-edab-481b-8818-5e153737c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_sch = xr.open_zarr(\"gs://sam-temp-dev/schout_w7xb7_05.zarr\", consolidated=True)\n",
    "ds_sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224cf161-ae3c-42d8-903b-d9e9f6536f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "max_depth = ds_sch.isel(time=slice(0, 190)).elev.max(dim=\"time\") + ds_sch.isel(time=0).depth\n",
    "z = max_depth.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbfabdc-f953-466f-af7f-564c2ce9eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d2af5-a04f-4920-b67b-4e60899a118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = ds_sch.isel(time=0).SCHISM_hgrid_node_x.values\n",
    "y = ds_sch.isel(time=0).SCHISM_hgrid_node_y.values\n",
    "faces = ds_sch.isel(time=0).SCHISM_hgrid_face_nodes.values-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a7603-0b36-4a6d-b6ec-4aebeaf123d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "PLINTH = 0.01\n",
    "cities = [\"Chiba\"]\n",
    "POLYS_107 = \"/Users/slamont/japan_gis/geo_boundaries_shp/cities_107_with_grid_num.geojson\"\n",
    "\n",
    "gv_map = trimesh_max_depth_map(x, y, z, faces, \"Zarr-based Map\")\n",
    "gv_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dae89b-3cbe-43ca-83e5-efd6fa1dc7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimesh_max_depth_map(x, y, z, faces, title):\n",
    "\n",
    "    # Build the standard static map (max depth over some interval)\n",
    "    z[z < PLINTH] = np.nan\n",
    "\n",
    "    df_verts = pd.DataFrame({'x': x, 'y': y, 'z': z})\n",
    "    df_tris = pd.DataFrame(faces[:, 0:3],columns=['v0','v1','v2'])\n",
    "\n",
    "    gv_basemap = gv.tile_sources.CartoLight\n",
    "\n",
    "    gdf_107 = gpd.read_file(POLYS_107)\n",
    "    mask = gdf_107[\"E-Name\"].isin(cities)\n",
    "    gdf_107 = gdf_107[mask]\n",
    "    xmin, ymin, xmax, ymax = gdf_107.total_bounds\n",
    "    gv_poly_107 = gv.Polygons(gdf_107).opts(alpha=0.15)\n",
    "\n",
    "\n",
    "    gv_trimap = gv_basemap * gv_poly_107 * rasterize(gv.TriMesh((df_tris, df_verts), crs=ccrs.PlateCarree())).options(\n",
    "        cmap=cm.Spectral_r,\n",
    "        colorbar=True,\n",
    "        clim=(PLINTH, 10.),\n",
    "        clabel='meter',\n",
    "        width=520,\n",
    "        height=440,\n",
    "        title=f'{title}',\n",
    "        tools=['hover']).redim.range(x=(xmin, xmax),y=(ymin, ymax))\n",
    "\n",
    "    return gv_trimap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
